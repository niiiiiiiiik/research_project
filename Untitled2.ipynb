{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d37b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64dc89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df=pd.read_csv(\"cleaned_df_with_temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4966e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, force_dense=False):\n",
    "    if force_dense:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('to_dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Overall Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06327388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['match_id', 'team_1', 'team_2', 'batting_team', 'bowling_team', 'ball',\n",
      "       'batsman', 'bowler', 'runs', 'player_dismissed', 'city', 'venue',\n",
      "       'Date', 'toss_winner', 'toss_decision', 'match_winner',\n",
      "       'player_of_the_match', 'current_score', 'over', 'ball_no',\n",
      "       'balls_bowled', 'balls_left', 'wickets_left', 'crr', 'last_five',\n",
      "       'run_rate', 'wickets_rate', 'pressure_factor', 'toss_winner_wins',\n",
      "       'max_tempC', 'min_tempC', 'sun_hour'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_df.rename(columns={'Team 1': 'team_1', 'Team 2': 'team_2'}, inplace=True)\n",
    "\n",
    "# Check the new column names to confirm they have been renamed\n",
    "print(cleaned_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2717f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVC\n",
      "Overall Accuracy: 0.9664936290703162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       270\n",
      " Afghanistan       1.00      1.00      1.00       156\n",
      "   Australia       0.98      0.97      0.97      1574\n",
      "  Bangladesh       0.92      0.91      0.92       340\n",
      "     England       1.00      0.95      0.97      1475\n",
      "       India       0.99      0.98      0.99      1856\n",
      " New Zealand       0.92      1.00      0.96      1302\n",
      "    Pakistan       0.97      0.97      0.97      1820\n",
      "South Africa       0.98      0.97      0.97      1476\n",
      "   Sri Lanka       0.94      0.94      0.94      1289\n",
      " West Indies       0.96      0.96      0.96      1156\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.96      0.97      0.96     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: DecisionTreeClassifier\n",
      "Overall Accuracy: 0.9743589743589743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       270\n",
      " Afghanistan       1.00      1.00      1.00       156\n",
      "   Australia       0.98      0.97      0.97      1574\n",
      "  Bangladesh       1.00      0.91      0.96       340\n",
      "     England       0.98      0.96      0.97      1475\n",
      "       India       1.00      0.98      0.99      1856\n",
      " New Zealand       0.94      1.00      0.97      1302\n",
      "    Pakistan       0.97      0.98      0.98      1820\n",
      "South Africa       0.98      0.98      0.98      1476\n",
      "   Sri Lanka       0.95      0.97      0.96      1289\n",
      " West Indies       0.98      0.95      0.96      1156\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.97      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: GradientBoostingClassifier\n",
      "Overall Accuracy: 0.974044360547428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       270\n",
      " Afghanistan       1.00      1.00      1.00       156\n",
      "   Australia       0.98      0.97      0.97      1574\n",
      "  Bangladesh       0.92      1.00      0.96       340\n",
      "     England       0.97      0.97      0.97      1475\n",
      "       India       0.99      1.00      0.99      1856\n",
      " New Zealand       0.98      0.96      0.97      1302\n",
      "    Pakistan       0.96      1.00      0.98      1820\n",
      "South Africa       0.98      0.97      0.98      1476\n",
      "   Sri Lanka       0.98      0.94      0.96      1289\n",
      " West Indies       0.98      0.96      0.97      1156\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.98      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model: GaussianNB\n",
      "Overall Accuracy: 0.4995280792826805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      1.00      0.33       270\n",
      " Afghanistan       0.86      1.00      0.93       156\n",
      "   Australia       0.50      0.69      0.58      1574\n",
      "  Bangladesh       0.44      0.93      0.60       340\n",
      "     England       0.67      0.53      0.59      1475\n",
      "       India       1.00      0.22      0.36      1856\n",
      " New Zealand       0.34      0.76      0.47      1302\n",
      "    Pakistan       0.79      0.38      0.52      1820\n",
      "South Africa       0.79      0.20      0.31      1476\n",
      "   Sri Lanka       0.68      0.36      0.47      1289\n",
      " West Indies       0.47      0.77      0.58      1156\n",
      "\n",
      "    accuracy                           0.50     12714\n",
      "   macro avg       0.61      0.62      0.52     12714\n",
      "weighted avg       0.66      0.50      0.49     12714\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Convert match_winner to string type to ensure consistency\n",
    "cleaned_df['match_winner'] = cleaned_df['match_winner'].astype(str)\n",
    "\n",
    "categorical_features = ['venue', 'team_1','team_2','batting_team', 'bowling_team', 'toss_decision', 'toss_winner']\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour']\n",
    "\n",
    "# Column Transformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "X = cleaned_df.drop(['match_id', 'match_winner'], axis=1)\n",
    "y = cleaned_df['match_winner']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Models and whether they need dense data\n",
    "models = [\n",
    "    (SVC(kernel='rbf', random_state=42), False),\n",
    "    (DecisionTreeClassifier(random_state=42), False),\n",
    "    (GradientBoostingClassifier(n_estimators=100, random_state=42), False),\n",
    "    (GaussianNB(), True)\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model, needs_dense in models:\n",
    "    train_and_evaluate(model, X_train, X_test, y_train, y_test, force_dense=needs_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0125f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97256907 0.97306066 0.97374889 0.97404385 0.97374889]\n",
      "Average cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Define the categorical and numerical features\n",
    "categorical_features = ['venue', 'team_1', 'team_2', 'batting_team', 'bowling_team', 'toss_decision', 'toss_winner']\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "X = cleaned_df.drop(['match_id', 'match_winner'], axis=1)\n",
    "y = cleaned_df['match_winner']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5)  \n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef41738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.974044360547428\n",
      "Classification report on test set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95       270\n",
      " Afghanistan       1.00      1.00      1.00       156\n",
      "   Australia       0.98      0.97      0.97      1574\n",
      "  Bangladesh       0.92      1.00      0.96       340\n",
      "     England       0.97      0.97      0.97      1475\n",
      "       India       0.99      1.00      0.99      1856\n",
      " New Zealand       0.98      0.96      0.97      1302\n",
      "    Pakistan       0.96      1.00      0.98      1820\n",
      "South Africa       0.98      0.97      0.98      1476\n",
      "   Sri Lanka       0.98      0.94      0.96      1289\n",
      " West Indies       0.98      0.96      0.97      1156\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.98      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification report on test set:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df58769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation F1-scores: [0.97249922 0.97296179 0.97367723 0.97395363 0.97369493]\n",
      "Average F1-score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Using F1-score as the scoring metric\n",
    "f1_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=make_scorer(f1_score, average='weighted'))\n",
    "print(\"Cross-validation F1-scores:\", f1_scores)\n",
    "print(\"Average F1-score: {:.2f}\".format(f1_scores.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026474c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data=pd.read_csv(\"cleaned_df_with_temp_not_used.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df22b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['match_id', 'team_1', 'team_2', 'batting_team', 'bowling_team', 'ball',\n",
      "       'batsman', 'bowler', 'runs', 'player_dismissed', 'city', 'venue',\n",
      "       'Date', 'toss_winner', 'toss_decision', 'match_winner',\n",
      "       'player_of_the_match', 'current_score', 'over', 'ball_no',\n",
      "       'balls_bowled', 'balls_left', 'wickets_left', 'crr', 'last_five',\n",
      "       'run_rate', 'wickets_rate', 'pressure_factor', 'max_tempC', 'min_tempC',\n",
      "       'sun_hour'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unseen_data.rename(columns={'Team 1': 'team_1', 'Team 2': 'team_2'}, inplace=True)\n",
    "\n",
    "# Check the new column names to confirm they have been renamed\n",
    "print(unseen_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7011c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen data: 0.67\n",
      "\n",
      "Classification Report on Unseen Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       127\n",
      " Afghanistan       1.00      0.50      0.67       490\n",
      "   Australia       0.85      0.85      0.85       830\n",
      "  Bangladesh       0.00      0.00      0.00       247\n",
      "     England       0.50      0.67      0.57       374\n",
      "       India       0.89      1.00      0.94       995\n",
      " New Zealand       0.51      0.33      0.40       766\n",
      "    Pakistan       0.36      1.00      0.53       359\n",
      "South Africa       0.64      0.78      0.70       572\n",
      "   Sri Lanka       0.57      1.00      0.72       488\n",
      " West Indies       1.00      0.34      0.50       746\n",
      "\n",
      "    accuracy                           0.67      5994\n",
      "   macro avg       0.57      0.59      0.54      5994\n",
      "weighted avg       0.70      0.67      0.64      5994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nikhil/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nikhil/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Extracting features from the unseen data\n",
    "X_unseen = unseen_data.drop(['match_id', 'match_winner'], axis=1)  \n",
    "y_unseen = unseen_data['match_winner']  \n",
    "\n",
    "# Predict using the trained pipeline\n",
    "y_pred_unseen = pipeline.predict(X_unseen)\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_unseen, y_pred_unseen)\n",
    "print(f\"Accuracy on unseen data: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report on Unseen Data:\")\n",
    "print(classification_report(y_unseen, y_pred_unseen))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb5cd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy on unseen data: 0.59\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Predict on unseen data\n",
    "y_pred_unseen = pipeline.predict(X_unseen)\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(y_unseen, y_pred_unseen)\n",
    "print(f\"Balanced Accuracy on unseen data: {balanced_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a56b7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9752241623407267\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      6560\n",
      "           1       0.97      0.97      0.97      6154\n",
      "\n",
      "    accuracy                           0.98     12714\n",
      "   macro avg       0.98      0.98      0.98     12714\n",
      "weighted avg       0.98      0.98      0.98     12714\n",
      "\n",
      "Feature Importance:\n",
      "\n",
      "team_2: 0.1118\n",
      "team_1: 0.0998\n",
      "venue: 0.0944\n",
      "toss_winner: 0.0634\n",
      "max_tempC: 0.0375\n",
      "toss_decision: 0.0361\n",
      "min_tempC: 0.0116\n",
      "sun_hour: 0.0064\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "\n",
    "# Define the binary outcome\n",
    "cleaned_df['match_outcome'] = (cleaned_df['team_1'] == cleaned_df['match_winner']).astype(int)\n",
    "\n",
    "# Select features, ensuring no feature leakage\n",
    "features_to_use = ['venue', 'team_1', 'team_2', 'toss_winner', 'toss_decision', 'max_tempC', 'min_tempC', 'sun_hour']\n",
    "X = cleaned_df[features_to_use]\n",
    "y = cleaned_df['match_outcome']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing steps for numerical and categorical data\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour']\n",
    "categorical_features = ['venue', 'team_1', 'team_2', 'toss_winner', 'toss_decision']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline setup\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Analyze feature importance\n",
    "results = permutation_importance(pipeline, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "importance_sorted = sorted(zip(results.importances_mean, features_to_use), reverse=True)\n",
    "print(\"Feature Importance:\\n\")\n",
    "for importance, name in importance_sorted:\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58446908",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data['match_outcome'] = (unseen_data['team_1'] == unseen_data['match_winner']).astype(int)  # Only if outcome needs to be checked\n",
    "features_to_use = ['venue', 'team_1', 'team_2', 'toss_winner', 'toss_decision', 'max_tempC', 'min_tempC', 'sun_hour']\n",
    "X_unseen = unseen_data[features_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19cad5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outcomes on the unseen data\n",
    "y_unseen_pred = pipeline.predict(X_unseen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74894ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen data: 0.594260927594261\n",
      "Classification report on unseen data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.74      0.54      1946\n",
      "           1       0.81      0.52      0.63      4048\n",
      "\n",
      "    accuracy                           0.59      5994\n",
      "   macro avg       0.62      0.63      0.59      5994\n",
      "weighted avg       0.69      0.59      0.61      5994\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1449  497]\n",
      " [1935 2113]]\n"
     ]
    }
   ],
   "source": [
    "# If actual outcomes are known and included in unseen_data\n",
    "y_unseen_actual = unseen_data['match_outcome']\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy on unseen data:\", accuracy_score(y_unseen_actual, y_unseen_pred))\n",
    "print(\"Classification report on unseen data:\\n\", classification_report(y_unseen_actual, y_unseen_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_unseen_actual, y_unseen_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3584e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom threshold - Accuracy on training data: 0.9772097138924393\n",
      "Custom threshold - Accuracy on test data: 0.9752241623407267\n"
     ]
    }
   ],
   "source": [
    "# First, ensure the prediction for training and unseen datasets are stored properly:\n",
    "y_train_proba = pipeline.predict_proba(X_train)[:, 1]  # Probabilities for 'team_1' winning on the training data\n",
    "y_test_proba = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for 'team_1' winning on the test data\n",
    "\n",
    "# Applying a custom threshold of 0.5 (as an example, can be optimized)\n",
    "threshold = 0.5\n",
    "y_train_pred_custom = (y_train_proba >= threshold).astype(int)\n",
    "y_test_pred_custom = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluate custom threshold predictions\n",
    "print(\"Custom threshold - Accuracy on training data:\", accuracy_score(y_train, y_train_pred_custom))\n",
    "print(\"Custom threshold - Accuracy on test data:\", accuracy_score(y_test, y_test_pred_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf8118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 0.9857074032439377\n"
     ]
    }
   ],
   "source": [
    "# Create a binary target based on whether the toss decision matched the match outcome\n",
    "# Assuming 'match_outcome' correctly indicates the winner\n",
    "cleaned_df['toss_effective'] = (cleaned_df['toss_decision'] == cleaned_df['match_outcome']).astype(int)\n",
    "\n",
    "# Filter to focus only on matches where the toss winner won the match\n",
    "toss_win_effective_df = cleaned_df[cleaned_df['toss_winner'] == cleaned_df['match_winner']]\n",
    "\n",
    "# Select features that might influence the decision\n",
    "features = ['venue', 'team_1', 'team_2', 'toss_winner', 'max_tempC', 'min_tempC', 'sun_hour']\n",
    "X = toss_win_effective_df[features]\n",
    "y = toss_win_effective_df['toss_decision']  # Use toss decision directly if modeling decision strategy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour']\n",
    "categorical_features = ['venue', 'team_1', 'team_2', 'toss_winner']\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Modeling pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the effectiveness of toss decisions on new data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Model accuracy on test set:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "706738e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = ['venue', 'team_1', 'team_2', 'toss_winner', 'max_tempC', 'min_tempC', 'sun_hour']\n",
    "# Prepare features from unseen data\n",
    "X_unseen = unseen_data[expected_columns]\n",
    "\n",
    "# Predict using the trained pipeline\n",
    "y_unseen_pred = pipeline.predict(X_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b55ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen data: 0.4657991324657991\n",
      "Classification Report on Unseen Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.23      0.44      0.30      1565\n",
      "       field       0.71      0.48      0.57      4429\n",
      "\n",
      "    accuracy                           0.47      5994\n",
      "   macro avg       0.47      0.46      0.43      5994\n",
      "weighted avg       0.58      0.47      0.50      5994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if actual outcomes are available and evaluate predictions\n",
    "if 'toss_decision' in unseen_data.columns:\n",
    "    y_unseen_actual = unseen_data['toss_decision']\n",
    "    accuracy_unseen = accuracy_score(y_unseen_actual, y_unseen_pred)\n",
    "    print(\"Accuracy on unseen data:\", accuracy_unseen)\n",
    "    print(\"Classification Report on Unseen Data:\\n\", classification_report(y_unseen_actual, y_unseen_pred))\n",
    "else:\n",
    "    print(\"Predictions made on unseen data, but no actual outcomes provided for accuracy assessment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95430e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model accuracy on test set: 0.96916784646846\n",
      "Classification Report on Improved Model:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.96      0.97      0.97      5814\n",
      "       field       0.97      0.97      0.97      6900\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.97      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "\n",
    "# Feature engineering: Adding team win rates\n",
    "# Calculate win rates for Team 1 and Team 2 when they decided to bat first and won\n",
    "# Assuming 'toss_decision' can be 'bat' or 'field' and 'match_winner' indicates the winning team\n",
    "\n",
    "# For Team 1\n",
    "cleaned_df['team_1_win_rate_bat_first'] = cleaned_df.apply(\n",
    "    lambda row: 1 if (row['toss_winner'] == row['team_1'] and \n",
    "                      row['toss_decision'] == 'bat' and \n",
    "                      row['match_winner'] == row['team_1']) else 0, axis=1)\n",
    "cleaned_df['team_1_win_rate_bat_first'] = cleaned_df.groupby('team_1')['team_1_win_rate_bat_first'].transform('mean')\n",
    "\n",
    "# For Team 2\n",
    "cleaned_df['team_2_win_rate_bat_first'] = cleaned_df.apply(\n",
    "    lambda row: 1 if (row['toss_winner'] == row['team_2'] and \n",
    "                      row['toss_decision'] == 'bat' and \n",
    "                      row['match_winner'] == row['team_2']) else 0, axis=1)\n",
    "cleaned_df['team_2_win_rate_bat_first'] = cleaned_df.groupby('team_2')['team_2_win_rate_bat_first'].transform('mean')\n",
    "\n",
    "# Prepare features and target\n",
    "features = [\n",
    "    'venue', 'team_1', 'team_2', 'toss_winner', 'max_tempC', 'min_tempC', 'sun_hour',\n",
    "    'team_1_win_rate_bat_first', 'team_2_win_rate_bat_first'\n",
    "]\n",
    "X = cleaned_df[features]\n",
    "y = cleaned_df['toss_decision']\n",
    "\n",
    "# Balance the dataset if it's heavily skewed\n",
    "if y.value_counts(normalize=True).max() > 0.7:  # assuming imbalance if >70% in one class\n",
    "    # Resample to balance bat and field choices\n",
    "    X_minority = X[y == y.value_counts().idxmin()]\n",
    "    y_minority = y[y == y.value_counts().idxmin()]\n",
    "    X_majority = X[y == y.value_counts().idxmax()]\n",
    "    y_majority = y[y == y.value_counts().idxmax()]\n",
    "    X_minority_upsampled, y_minority_upsampled = resample(X_minority, y_minority, \n",
    "                                                          replace=True, \n",
    "                                                          n_samples=len(X_majority), \n",
    "                                                          random_state=42)\n",
    "    X_balanced = pd.concat([X_majority, X_minority_upsampled])\n",
    "    y_balanced = pd.concat([y_majority, y_minority_upsampled])\n",
    "else:\n",
    "    X_balanced, y_balanced = X, y\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing and model pipeline\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour', 'team_1_win_rate_bat_first', 'team_2_win_rate_bat_first']\n",
    "categorical_features = ['venue', 'team_1', 'team_2', 'toss_winner']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Improved Model accuracy on test set:\", accuracy)\n",
    "print(\"Classification Report on Improved Model:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d9d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Team 1\n",
    "unseen_data['team_1_win_rate_bat_first'] = unseen_data.apply(\n",
    "    lambda row: 1 if (row['toss_winner'] == row['team_1'] and \n",
    "                      row['toss_decision'] == 'bat' and \n",
    "                      row['match_winner'] == row['team_1']) else 0, axis=1)\n",
    "unseen_data['team_1_win_rate_bat_first'] = unseen_data.groupby('team_1')['team_1_win_rate_bat_first'].transform('mean')\n",
    "\n",
    "# For Team 2\n",
    "unseen_data['team_2_win_rate_bat_first'] = unseen_data.apply(\n",
    "    lambda row: 1 if (row['toss_winner'] == row['team_2'] and \n",
    "                      row['toss_decision'] == 'bat' and \n",
    "                      row['match_winner'] == row['team_2']) else 0, axis=1)\n",
    "unseen_data['team_2_win_rate_bat_first'] = unseen_data.groupby('team_2')['team_2_win_rate_bat_first'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9310fbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen data: 0.6421421421421422\n",
      "Classification Report on Unseen Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.39      0.68      0.50      1565\n",
      "       field       0.85      0.63      0.72      4429\n",
      "\n",
      "    accuracy                           0.64      5994\n",
      "   macro avg       0.62      0.65      0.61      5994\n",
      "weighted avg       0.73      0.64      0.66      5994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare the features for the unseen data\n",
    "X_unseen = unseen_data[['venue', 'team_1', 'team_2', 'toss_winner', 'max_tempC', 'min_tempC', 'sun_hour',\n",
    "                        'team_1_win_rate_bat_first', 'team_2_win_rate_bat_first']]\n",
    "\n",
    "# Predict the toss decisions on the unseen data\n",
    "y_unseen_pred = pipeline.predict(X_unseen)\n",
    "\n",
    "# If you have the actual toss decisions in the unseen data, evaluate predictions\n",
    "if 'toss_decision' in unseen_data.columns:\n",
    "    y_unseen_actual = unseen_data['toss_decision']\n",
    "    accuracy_unseen = accuracy_score(y_unseen_actual, y_unseen_pred)\n",
    "    print(\"Accuracy on unseen data:\", accuracy_unseen)\n",
    "    print(\"Classification Report on Unseen Data:\\n\", classification_report(y_unseen_actual, y_unseen_pred))\n",
    "else:\n",
    "    print(\"Predictions made on unseen data, but no actual outcomes provided for accuracy assessment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f07cb5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Results:\n",
      "CV Mean Accuracy: 0.9715\n",
      "Test Accuracy: 0.9692\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.97      0.96      0.97      5814\n",
      "       field       0.97      0.98      0.97      6900\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.97      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "DecisionTree Results:\n",
      "CV Mean Accuracy: 0.9711\n",
      "Test Accuracy: 0.9692\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.96      0.97      0.97      5814\n",
      "       field       0.97      0.97      0.97      6900\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.97      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "GradientBoosting Results:\n",
      "CV Mean Accuracy: 0.8147\n",
      "Test Accuracy: 0.8177\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.84      0.74      0.79      5814\n",
      "       field       0.80      0.88      0.84      6900\n",
      "\n",
      "    accuracy                           0.82     12714\n",
      "   macro avg       0.82      0.81      0.81     12714\n",
      "weighted avg       0.82      0.82      0.82     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "GaussianNB Results:\n",
      "CV Mean Accuracy: 0.5510\n",
      "Test Accuracy: 0.5485\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.50      1.00      0.67      5814\n",
      "       field       1.00      0.17      0.29      6900\n",
      "\n",
      "    accuracy                           0.55     12714\n",
      "   macro avg       0.75      0.58      0.48     12714\n",
      "weighted avg       0.77      0.55      0.46     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "LogisticRegression Results:\n",
      "CV Mean Accuracy: 0.7161\n",
      "Test Accuracy: 0.7168\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.69      0.69      0.69      5814\n",
      "       field       0.74      0.74      0.74      6900\n",
      "\n",
      "    accuracy                           0.72     12714\n",
      "   macro avg       0.71      0.71      0.71     12714\n",
      "weighted avg       0.72      0.72      0.72     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "RandomForest Results:\n",
      "CV Mean Accuracy: 0.9711\n",
      "Test Accuracy: 0.9692\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.96      0.97      0.97      5814\n",
      "       field       0.97      0.97      0.97      6900\n",
      "\n",
      "    accuracy                           0.97     12714\n",
      "   macro avg       0.97      0.97      0.97     12714\n",
      "weighted avg       0.97      0.97      0.97     12714\n",
      "\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Dictionary to store trained pipelines for later use\n",
    "trained_pipelines = {}\n",
    "\n",
    "# Define features and target\n",
    "features = ['venue', 'team_1', 'team_2', 'toss_winner', 'max_tempC', 'min_tempC', 'sun_hour',\n",
    "            'team_1_win_rate_bat_first', 'team_2_win_rate_bat_first']\n",
    "X = cleaned_df[features]\n",
    "y = cleaned_df['toss_decision']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numerical_features = ['max_tempC', 'min_tempC', 'sun_hour', 'team_1_win_rate_bat_first', 'team_2_win_rate_bat_first']\n",
    "categorical_features = ['venue', 'team_1', 'team_2', 'toss_winner']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define a transformer to convert sparse output to dense for models that need it\n",
    "to_dense_transformer = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n",
    "\n",
    "# Function to get the right pipeline based on the model\n",
    "def get_model_pipeline(model):\n",
    "    if isinstance(model, GaussianNB):\n",
    "        # Include conversion to dense array for GaussianNB\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('to_dense', to_dense_transformer),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    else:\n",
    "        # Other models can handle sparse data directly\n",
    "        return Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\n",
    "    ('SVC', SVC(kernel='rbf', random_state=42)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('GradientBoosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models:\n",
    "    pipeline = get_model_pipeline(model)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'CV Mean Accuracy': cv_scores.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Classification Report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "    trained_pipelines[name] = pipeline \n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name} Results:\")\n",
    "    for key, value in metrics.items():\n",
    "        if key == 'Classification Report':\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "        else:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00acf301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVC on Unseen Data:\n",
      "Accuracy: 0.6181\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.38      0.77      0.51      1565\n",
      "       field       0.87      0.56      0.69      4429\n",
      "\n",
      "    accuracy                           0.62      5994\n",
      "   macro avg       0.63      0.67      0.60      5994\n",
      "weighted avg       0.75      0.62      0.64      5994\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for DecisionTree on Unseen Data:\n",
      "Accuracy: 0.5834\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.33      0.60      0.43      1565\n",
      "       field       0.80      0.58      0.67      4429\n",
      "\n",
      "    accuracy                           0.58      5994\n",
      "   macro avg       0.57      0.59      0.55      5994\n",
      "weighted avg       0.68      0.58      0.61      5994\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for GradientBoosting on Unseen Data:\n",
      "Accuracy: 0.5794\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.30      0.44      0.35      1565\n",
      "       field       0.76      0.63      0.69      4429\n",
      "\n",
      "    accuracy                           0.58      5994\n",
      "   macro avg       0.53      0.53      0.52      5994\n",
      "weighted avg       0.64      0.58      0.60      5994\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for GaussianNB on Unseen Data:\n",
      "Accuracy: 0.2806\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.26      0.92      0.40      1565\n",
      "       field       0.66      0.05      0.10      4429\n",
      "\n",
      "    accuracy                           0.28      5994\n",
      "   macro avg       0.46      0.49      0.25      5994\n",
      "weighted avg       0.55      0.28      0.18      5994\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for LogisticRegression on Unseen Data:\n",
      "Accuracy: 0.6298\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.30      0.31      0.31      1565\n",
      "       field       0.75      0.74      0.75      4429\n",
      "\n",
      "    accuracy                           0.63      5994\n",
      "   macro avg       0.53      0.53      0.53      5994\n",
      "weighted avg       0.63      0.63      0.63      5994\n",
      "\n",
      "--------------------------------------------------\n",
      "Results for RandomForest on Unseen Data:\n",
      "Accuracy: 0.6421\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bat       0.39      0.68      0.50      1565\n",
      "       field       0.85      0.63      0.72      4429\n",
      "\n",
      "    accuracy                           0.64      5994\n",
      "   macro avg       0.62      0.65      0.61      5994\n",
      "weighted avg       0.73      0.64      0.66      5994\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prepare the features for unseen data just like the training data\n",
    "X_unseen = unseen_data[features]  # Ensure all necessary features are included\n",
    "\n",
    "# The actual outcomes for evaluation\n",
    "y_unseen = unseen_data['toss_decision']  # This is our target column in the unseen data\n",
    "\n",
    "# Define a dictionary to store each model's evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Use each trained pipeline to predict and evaluate on the unseen data\n",
    "for name, pipeline in trained_pipelines.items():\n",
    "    # Prediction on unseen data\n",
    "    y_unseen_pred = pipeline.predict(X_unseen)\n",
    "    \n",
    "    # Calculate accuracy and generate a classification report\n",
    "    accuracy_unseen = accuracy_score(y_unseen, y_unseen_pred)\n",
    "    classification_rep = classification_report(y_unseen, y_unseen_pred)\n",
    "    \n",
    "    # Store results\n",
    "    evaluation_results[name] = {\n",
    "        'Accuracy on Unseen Data': accuracy_unseen,\n",
    "        'Classification Report on Unseen Data': classification_rep\n",
    "    }\n",
    "\n",
    "# Print results for models on unseen data\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    print(f\"Results for {model_name} on Unseen Data:\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy on Unseen Data']:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics['Classification Report on Unseen Data'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b53cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
